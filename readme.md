# ğŸ§  Virtual Human Interface â€“ BTU Buddy

A realâ€‘time, voiceâ€‘driven **Virtual Human** system built for interactive humanâ€‘computer communication. The system captures spoken input, transcribes it with Whisper, generates contextâ€‘aware responses using OpenAI GPT, converts them to lifelike speech with Coquiâ€¯TTS, and sends facial blendshape data to Unreal Engine for animated avatar rendering.

---

## ğŸš€ TechÂ Stack

| Layer             | Technology                   | Purpose                              |
| ----------------- | ---------------------------- | ------------------------------------ |
| **Input & Audio** | `sounddevice`, `simpleaudio` | Microphone capture & playback        |
| **ASR**           | **OpenAIÂ Whisper**           | Speechâ€‘toâ€‘text transcription         |
| **LLM**           | **OpenAIâ€¯GPTâ€‘3.5â€‘Turbo**     | Naturalâ€‘language response generation |
| **TTS**           | **Coquiâ€¯TTS**                | Neural textâ€‘toâ€‘speech synthesis      |
| **Animation**     | ARKitâ€‘style blendshapes      | Facial expression parameters         |
| **GameÂ Engine**   | **UnrealÂ EngineÂ 5**          | Avatar animation & rendering         |
| **Communication** | HTTPÂ `POST` (`requests`)     | PythonÂ â†’Â Unreal data transfer        |
| **Environment**   | Pythonâ€¯3.10Â (Conda)          | Runtime platform                     |

---

## ğŸ§© ProjectÂ Structure

```
Virtual_Human/
â”œâ”€â”€ main.py                 # Orchestrates all modules
â”œâ”€â”€ asr.py                  # Whisperâ€‘based transcription
â”œâ”€â”€ nlp.py                  # OpenAI GPT integration
â”œâ”€â”€ tts.py                  # CoquiÂ TTS synthesis & playback
â”œâ”€â”€ lip.py                  # Generates ARKit blendshapes
â”œâ”€â”€ captured_audio.wav      # Temporary audio storage
â”œâ”€â”€ requirements.txt        # All Python deps
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ› ï¸Â SetupÂ Instructions

### 1Â â€”Â CloneÂ theÂ repository

```bash
git clone https://github.com/iamtarunshyam/Virtual_Human.git
cd Virtual_Human
```

### 2Â â€”Â CreateÂ &Â activateÂ CondaÂ environment *(PythonÂ 3.10)*

```bash
conda create -n virtualhuman python=3.10 -y
conda activate virtualhuman
```

### 3Â â€”Â InstallÂ dependencies

```bash
pip install -r requirements.txt
```

> **NoteÂ :**Â `TTS` and `whisper` currently require PythonÂ â‰¤3.10.

### 4Â â€”Â SetÂ yourÂ OpenAIÂ APIÂ Key

```bash
export OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXX
```

*or* create a `.env` file:

```
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXX
```

---

## â–¶ï¸Â RunningÂ theÂ Application

```bash
python main.py
```

The system will:

1. Welcome you via TTS.
2. Listen for your spoken question.
3. Generate a GPT response.
4. Speak the answer aloud.
5. Send blendshape data to Unreal for avatar animation.

---

## ğŸ”„ UnrealÂ EngineÂ Integration

`lip.py` posts ARKit blendshape JSON to:

```
http://localhost:8080/apply_blendshapes
```

Make sure Unreal Engine (or OmniverseÂ â†’Â Unreal bridge) is running an HTTP listener on that endpointâ€”either via **RemoteÂ ControlÂ API**, a Python script, or plugins such as **VaRest** / **WebÂ RemoteÂ Control**.

---

## ğŸ“¦ Dependencies (excerpt)

```
openai==0.28.1
TTS
whisper
simpleaudio
sounddevice
requests
python-dotenv
```

Install all with:

```bash
pip install -r requirements.txt
```

---

## ğŸŒ .envÂ Template

Create `.env` or copy from `.env.example`:

```
OPENAI_API_KEY=your_api_key_here
```

---

## ğŸ“¸Â Demo

> *(Add screenshots or demo video/GIF here)*

---

## ğŸ“„Â License

MITÂ License â€”â€¯free for academic and research use.

---

## ğŸ™‹â€â™‚ï¸Â Author

**TarunÂ Shyam** Â |Â  MScÂ AI, BTUÂ Cottbus Â |Â  [@iamtarunshyam](https://github.com/iamtarunshyam)

